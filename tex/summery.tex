\documentclass{article}
\title{Linear models}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algpseudocode,algorithm} 
\input{macros}
\author{Uri Heinemann}
\begin{document}
\maketitle
In this project 5 files of data was given.
Each file contain two columns $x$, $y$.
It was said that $y$ was generated by $y = ax+b+\epsilon(x)$ where $a,b$ are constant for each file (vary between files) and $\epsilon(x)$ is a random variable that may depend on $x$ from unknown distribution. The distribution is fixed a cross the files and have zero mean given $x$.

In order to find $a,b$ we need to find suitable noise distribution such it will best explain the given data.
For this purpose we tried 4 different noise distributions.
Each noise distribution is a class in the given code.
We used maximum likelihood as the objective for parameters learning.
First initial values were given for all parameters.
Than for each file different $a,b$ was learned.
Having that the ``noise'' was collected from all files ($ax+b-y$) and the parameters of the noise distribution were learned.
Finally $a,b$ were re-learned using the new weights (The different noise distribution induce different weights of the samples). 
In order to assess the quality of the model the likelihood of a test set was calculated - the model with the maximum test likelihood is chosen.
in the following I will describe the different models and at the end explain the results.
\section{Normal Distribution}
The first distribution we test was the simplest - the normal distribution.
Since the noise may depend on $x$ the variance of the normal distribution may depend on $x$.
Remembering that the variance must be positive, we model it by $\sigma^2(x) = \mexp{c x + d}$.
Giving that, $y$ is now distributed as $y \sim N(ax+b, \mexp{cx+d})$, result with the following likelihood
\be
\frac{1}{n} \sum_i \log p(y_i | x_i) = \frac{1}{2n} \sum_i -(c x_i +d)-\log{2\pi} - (a x_i + b - y_i)^2 \mexp{-(c x_i + d)}
\ee

This is indeed the most trivial model, and is known to suffer from sensitivity to outliers.
But is is clear that the noise is indeed depend on $x$ since we can see a significance increase in the likelihood after $c,d$ are learned where $c$ is far from zero.
\section{Exponential}
We were now after a distribution that less sensitive to outliers.
One such distribution is the exponential where its pdf is defined as $p(x;\lambda) = \lambda \mexp{-\lambda x}$ but it is defined only for positive $x>0$ values ($\lambda >0$ as well).
In order to allow mistakes for both sides, we added another simple random variable that choose the sign of the error with equal probability.
Note that by this we assume symmetric noise - assumption that exists in the normal case as well.
Giving the option for the rate ($\lambda$) to depend on $x$ - $\lambda = \mexp{cx + d}$ results $p(y|x;a,b,c,d) = \frac{1}{2}\mexp{cx+d}\mexp{-\mexp{cx+d}|ax+b-y|} $ with the likelihood:
\be
\frac{1}{n} \sum_i \log p(y_i|x_i,a,b,c,d) = \frac{1}{2n} \sum_i c x_i +d -\mexp{c x_i +d}|a x_i  + b - y_i |
\ee

The likelihood was greatly increased using this noise model, and as in the normal case dependence in $x$ farther increase the likelihood.
The downside is that now $a,b$ needed to be learned by convex optimization while in the normal case a closed form equations exists.
Note that in both cases the noise parameters are convex optimization.

\section{Pareto distribution}
Our last trail was with a heavy tailed distribution Pareto.
The pareto distribution is defined as $p(x;\lambda, \alpha) = \frac{\alpha}{\lambda}\left(\frac{x}{\lambda}\right)^{-(\alpha +1)}$ again $x$ and all the parameters must be positive moreover $x>\lambda>0$. We solved this as with the exponential distribution result with maximum likelihood:
\bea
&\frac{1}{n}& \sum_i \log p(y_i|x_i,a,b,c,d, e,f) = \\
&\frac{1}{2n}& \sum_i ex+f -(\mexp{ex+f}+ 1)\log(|a x_i + b -y_i|) + \mexp{ex+f}(c x_i+d)
\eea

We tried two versions for this distribution in the first in the first we set $e=0$ and learned only $\alpha = \mexp{f}$.
In the second both parameters where allowed to depend on $x$ but this result with a non convex optimization of the noise parameters.

\section{Conclusion}
The exponential distribution result with the highest likelihood.
The reason for this that in the Pareto case varying $\lambda$ is not strong enough and by allowing both parameters to depend on $x$ tyhe optimization failed - we were stuck in local maximum.
Farther work should try other noise distribution, remove the assumption of noise symmetric, check other models for dependency of $x$, etc.






\end{document}